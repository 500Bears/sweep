{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_TYPE\n",
      "259 274\n",
      "'NoneType' object has no attribute 'get_object'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     46\u001b[0m     \u001b[39mprint\u001b[39m(e)\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     myresource\u001b[39m.\u001b[39mwrite(contents)\n",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(start, end)\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     extractor \u001b[39m=\u001b[39m InlineMethod(myproject, myresource, start)\n\u001b[1;32m     36\u001b[0m     change_set \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mget_changes(\u001b[39m\"\u001b[39m\u001b[39mopenai_api_type\u001b[39m\u001b[39m\"\u001b[39m, similar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mfor\u001b[39;00m change \u001b[39min\u001b[39;00m change_set\u001b[39m.\u001b[39mchanges:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sweepai-u_CIt3kb-py3.10/lib/python3.10/site-packages/rope/refactor/inline.py:99\u001b[0m, in \u001b[0;36mInlineMethod.__init__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     98\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyfunction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyname\u001b[39m.\u001b[39;49mget_object()\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpymodule \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyfunction\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresource \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyfunction\u001b[39m.\u001b[39mget_module()\u001b[39m.\u001b[39mget_resource()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_object'"
     ]
    }
   ],
   "source": [
    "import rope.base.project\n",
    "from rope.refactor.extract import ExtractMethod\n",
    "from rope.refactor.inline import InlineMethod\n",
    "import re\n",
    "\n",
    "APOSTROPHE_MARKER = \"__APOSTROPHE__\"\n",
    "PERCENT_FORMAT_MARKER = \"__PERCENT_FORMAT__\"\n",
    "\n",
    "def serialize(text: str):\n",
    "    # Replace \"'{var}'\" with \"__APOSTROPHE__{var}__APOSTROPHE__\"\n",
    "    text = re.sub(r\"'{([^'}]*?)}'\", f\"{APOSTROPHE_MARKER}{{\\\\1}}{APOSTROPHE_MARKER}\", text)\n",
    "    # Replace \"%s\" with \"__PERCENT_FORMAT__\"\n",
    "    text = re.sub(r\"%\\((.*?)\\)s\", f\"{PERCENT_FORMAT_MARKER}{{\\\\1}}\", text)\n",
    "    return text\n",
    "\n",
    "def deserialize(text: str):\n",
    "    text = re.sub(f\"{APOSTROPHE_MARKER}{{(.*?)}}{APOSTROPHE_MARKER}\", \"'{\\\\1}'\", text)\n",
    "    text = re.sub(f\"{PERCENT_FORMAT_MARKER}{{(.*?)}}\", \"%(\\\\1)s\", text)\n",
    "    return text\n",
    "\n",
    "myproject = rope.base.project.Project('src')\n",
    "\n",
    "myresource = myproject.get_resource('mod3.py')\n",
    "contents = myresource.read()\n",
    "serialized_contents = serialize(myresource.read())\n",
    "myresource.write(serialized_contents)\n",
    "extract_span = r\"\"\"OPENAI_API_TYPE\"\"\"\n",
    "serialized_extract_span = serialize(extract_span)\n",
    "print(serialized_extract_span)\n",
    "\n",
    "start, end = serialized_contents.find(serialized_extract_span), serialized_contents.find(serialized_extract_span) + len(serialized_extract_span)\n",
    "print(start, end)\n",
    "\n",
    "try:\n",
    "    extractor = InlineMethod(myproject, myresource, start)\n",
    "    change_set = extractor.get_changes(\"openai_api_type\", similar=True)\n",
    "    for change in change_set.changes:\n",
    "        if change.old_contents is not None:\n",
    "            change.old_contents = deserialize(change.old_contents)\n",
    "        else:\n",
    "            change.old_contents = deserialize(change.resource.read())\n",
    "        change.new_contents = deserialize(change.new_contents)\n",
    "    for change in change_set.changes:\n",
    "        print(change.get_description())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise e\n",
    "finally:\n",
    "    myresource.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myresource = myproject.get_resource('mod3.py')\n",
    "contents = myresource.read()\n",
    "serialized_contents = serialize(myresource.read())\n",
    "myresource.write(serialized_contents)\n",
    "extract_span = r\"\"\"                openai.api_type = OPENAI_API_TYPE\n",
    "                openai.api_base = OPENAI_API_BASE\n",
    "                openai.api_version = OPENAI_API_VERSION\n",
    "                openai.api_key = AZURE_API_KEY\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    engine=engine,\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    timeout=OPENAI_TIMEOUT,\n",
    "                )\"\"\"\n",
    "serialized_extract_span = serialize(extract_span)\n",
    "print(serialized_extract_span)\n",
    "\n",
    "start, end = serialized_contents.find(serialized_extract_span), serialized_contents.find(serialized_extract_span) + len(serialized_extract_span)\n",
    "print(start, end)\n",
    "\n",
    "try:\n",
    "    extractor = ExtractMethod(myproject, myresource, start, end)\n",
    "    change_set = extractor.get_changes(\"helper\", similar=True)\n",
    "    for change in change_set.changes:\n",
    "        if change.old_contents is not None:\n",
    "            change.old_contents = deserialize(change.old_contents)\n",
    "        else:\n",
    "            change.old_contents = deserialize(change.resource.read())\n",
    "        change.new_contents = deserialize(change.new_contents)\n",
    "    for change in change_set.changes:\n",
    "        print(change.get_description())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    myresource.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(change.new_contents) - len(change.old_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restructuring <\n",
      "openai.api_key = ${api_key}\n",
      "openai.api_base = ${api_base}\n",
      "openai.api_version = ${api_version}\n",
      "openai.api_type = ${api_type}\n",
      "logger.info(f\"Calling {model} with OpenAI.\")\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=${model},\n",
      "    messages=${messages},\n",
      "    max_tokens=${max_tokens},\n",
      "    temperature=${temperature},\n",
      "    timeout=${timeout},\n",
      "    seed=${seed},\n",
      ")> to <print(\n",
      "    ${api_key},\n",
      "    ${api_base},\n",
      "    ${api_version},\n",
      "    ${api_type},\n",
      "    ${model},\n",
      "    ${messages},\n",
      "    ${max_tokens},\n",
      "    ${temperature},\n",
      "    ${timeout},\n",
      "    ${seed},\n",
      ")>:\n",
      "\n",
      "\n",
      "--- a/mod3.py\n",
      "+++ b/mod3.py\n",
      "@@ -29,18 +29,17 @@\n",
      "             ):\n",
      "                 engine = OPENAI_API_ENGINE_GPT4_32K\n",
      "             if OPENAI_API_TYPE is None or engine is None:\n",
      "-                openai.api_key = OPENAI_API_KEY\n",
      "-                openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                openai.api_version = None\n",
      "-                openai.api_type = \"open_ai\"\n",
      "-                logger.info(f\"Calling {model} with OpenAI.\")\n",
      "-                response = openai.ChatCompletion.create(\n",
      "-                    model=model,\n",
      "-                    messages=messages,\n",
      "-                    max_tokens=max_tokens,\n",
      "-                    temperature=temperature,\n",
      "-                    timeout=OPENAI_TIMEOUT,\n",
      "-                    seed=SEED,\n",
      "+                print(\n",
      "+                    OPENAI_API_KEY,\n",
      "+                    \"https://api.openai.com/v1\",\n",
      "+                    None,\n",
      "+                    \"open_ai\",\n",
      "+                    model,\n",
      "+                    messages,\n",
      "+                    max_tokens,\n",
      "+                    temperature,\n",
      "+                    OPENAI_TIMEOUT,\n",
      "+                    SEED,\n",
      "                 )\n",
      "                 return response[\"choices\"][0].message.content\n",
      "             # validity checks for MULTI_REGION_CONFIG\n",
      "@@ -100,18 +99,17 @@\n",
      "         except Exception as e:\n",
      "             if OPENAI_API_KEY:\n",
      "                 try:\n",
      "-                    openai.api_key = OPENAI_API_KEY\n",
      "-                    openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                    openai.api_version = None\n",
      "-                    openai.api_type = \"open_ai\"\n",
      "-                    logger.info(f\"Calling {model} with OpenAI.\")\n",
      "-                    response = openai.ChatCompletion.create(\n",
      "-                        model=model,\n",
      "-                        messages=messages,\n",
      "-                        max_tokens=max_tokens,\n",
      "-                        temperature=temperature,\n",
      "-                        timeout=OPENAI_TIMEOUT,\n",
      "-                        seed=SEED,\n",
      "+                    print(\n",
      "+                        OPENAI_API_KEY,\n",
      "+                        \"https://api.openai.com/v1\",\n",
      "+                        None,\n",
      "+                        \"open_ai\",\n",
      "+                        model,\n",
      "+                        messages,\n",
      "+                        max_tokens,\n",
      "+                        temperature,\n",
      "+                        OPENAI_TIMEOUT,\n",
      "+                        SEED,\n",
      "                     )\n",
      "                     return response[\"choices\"][0].message.content\n",
      "                 except SystemExit:\n",
      "\n",
      "--- a/test.py\n",
      "+++ b/test.py\n",
      "@@ -60,18 +60,17 @@\n",
      "             ):\n",
      "                 engine = OPENAI_API_ENGINE_GPT4_32K\n",
      "             if OPENAI_API_TYPE is None or engine is None:\n",
      "-                openai.api_key = OPENAI_API_KEY\n",
      "-                openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                openai.api_version = None\n",
      "-                openai.api_type = \"open_ai\"\n",
      "-                logger.info(f\"Calling {model} with OpenAI.\")\n",
      "-                response = openai.ChatCompletion.create(\n",
      "-                    model=model,\n",
      "-                    messages=messages,\n",
      "-                    max_tokens=max_tokens,\n",
      "-                    temperature=temperature,\n",
      "-                    timeout=OPENAI_TIMEOUT,\n",
      "-                    seed=SEED,\n",
      "+                print(\n",
      "+                    OPENAI_API_KEY,\n",
      "+                    \"https://api.openai.com/v1\",\n",
      "+                    None,\n",
      "+                    \"open_ai\",\n",
      "+                    model,\n",
      "+                    messages,\n",
      "+                    max_tokens,\n",
      "+                    temperature,\n",
      "+                    OPENAI_TIMEOUT,\n",
      "+                    SEED,\n",
      "                 )\n",
      "                 return response[\"choices\"][0].message.content\n",
      "             # validity checks for MULTI_REGION_CONFIG\n",
      "@@ -131,18 +130,17 @@\n",
      "         except Exception as e:\n",
      "             if OPENAI_API_KEY:\n",
      "                 try:\n",
      "-                    openai.api_key = OPENAI_API_KEY\n",
      "-                    openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                    openai.api_version = None\n",
      "-                    openai.api_type = \"open_ai\"\n",
      "-                    logger.info(f\"Calling {model} with OpenAI.\")\n",
      "-                    response = openai.ChatCompletion.create(\n",
      "-                        model=model,\n",
      "-                        messages=messages,\n",
      "-                        max_tokens=max_tokens,\n",
      "-                        temperature=temperature,\n",
      "-                        timeout=OPENAI_TIMEOUT,\n",
      "-                        seed=SEED,\n",
      "+                    print(\n",
      "+                        OPENAI_API_KEY,\n",
      "+                        \"https://api.openai.com/v1\",\n",
      "+                        None,\n",
      "+                        \"open_ai\",\n",
      "+                        model,\n",
      "+                        messages,\n",
      "+                        max_tokens,\n",
      "+                        temperature,\n",
      "+                        OPENAI_TIMEOUT,\n",
      "+                        SEED,\n",
      "                     )\n",
      "                     return response[\"choices\"][0].message.content\n",
      "                 except SystemExit:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myproject = rope.base.project.Project('./src')\n",
    "mod1 = myproject.get_resource('mod3.py')\n",
    "mod1.write(r\"\"\"\n",
    "class OpenAIProxy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @file_cache(ignore_params=[])\n",
    "    def call_openai(self, model, messages, max_tokens, temperature) -> str:\n",
    "        try:\n",
    "            engine = None\n",
    "            if model in OPENAI_EXCLUSIVE_MODELS and OPENAI_API_TYPE != \"azure\":\n",
    "                logger.info(f\"Calling OpenAI exclusive model. {model}\")\n",
    "                raise Exception(\"OpenAI exclusive model.\")\n",
    "            if (\n",
    "                model == \"gpt-3.5-turbo-16k\"\n",
    "                or model == \"gpt-3.5-turbo-16k-0613\"\n",
    "                and OPENAI_API_ENGINE_GPT35 is not None\n",
    "            ):\n",
    "                engine = OPENAI_API_ENGINE_GPT35\n",
    "            elif (\n",
    "                model == \"gpt-4\"\n",
    "                or model == \"gpt-4-0613\"\n",
    "                and OPENAI_API_ENGINE_GPT4 is not None\n",
    "            ):\n",
    "                engine = OPENAI_API_ENGINE_GPT4\n",
    "            elif (\n",
    "                model == \"gpt-4-32k\"\n",
    "                or model == \"gpt-4-32k-0613\"\n",
    "                and OPENAI_API_ENGINE_GPT4_32K is not None\n",
    "            ):\n",
    "                engine = OPENAI_API_ENGINE_GPT4_32K\n",
    "            if OPENAI_API_TYPE is None or engine is None:\n",
    "                openai.api_key = OPENAI_API_KEY\n",
    "                openai.api_base = \"https://api.openai.com/v1\"\n",
    "                openai.api_version = None\n",
    "                openai.api_type = \"open_ai\"\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    timeout=OPENAI_TIMEOUT,\n",
    "                    seed=SEED,\n",
    "                )\n",
    "                return response[\"choices\"][0].message.content\n",
    "            # validity checks for MULTI_REGION_CONFIG\n",
    "            if (\n",
    "                MULTI_REGION_CONFIG is None\n",
    "                or not isinstance(MULTI_REGION_CONFIG, list)\n",
    "                or len(MULTI_REGION_CONFIG) == 0\n",
    "                or not isinstance(MULTI_REGION_CONFIG[0], list)\n",
    "            ):\n",
    "                logger.info(\n",
    "                    f\"Calling {model} with engine {engine} on Azure url {OPENAI_API_BASE}.\"\n",
    "                )\n",
    "                openai.api_type = OPENAI_API_TYPE\n",
    "                openai.api_base = OPENAI_API_BASE\n",
    "                openai.api_version = OPENAI_API_VERSION\n",
    "                openai.api_key = AZURE_API_KEY\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    engine=engine,\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    timeout=OPENAI_TIMEOUT,\n",
    "                )\n",
    "                return response[\"choices\"][0].message.content\n",
    "            # multi region config is a list of tuples of (region_url, api_key)\n",
    "            # we will try each region in order until we get a response\n",
    "            # randomize the order of the list\n",
    "            SHUFFLED_MULTI_REGION_CONFIG = random.sample(\n",
    "                MULTI_REGION_CONFIG, len(MULTI_REGION_CONFIG)\n",
    "            )\n",
    "            for region_url, api_key in SHUFFLED_MULTI_REGION_CONFIG:\n",
    "                try:\n",
    "                    logger.info(\n",
    "                        f\"Calling {model} with engine {engine} on Azure url {region_url}.\"\n",
    "                    )\n",
    "                    openai.api_key = api_key\n",
    "                    openai.api_base = region_url\n",
    "                    openai.api_version = OPENAI_API_VERSION\n",
    "                    openai.api_type = OPENAI_API_TYPE\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        engine=engine,\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                        max_tokens=max_tokens,\n",
    "                        temperature=temperature,\n",
    "                        timeout=OPENAI_TIMEOUT,\n",
    "                    )\n",
    "                    return response[\"choices\"][0].message.content\n",
    "                except SystemExit:\n",
    "                    raise SystemExit\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Error calling {region_url}: {e}\")\n",
    "            raise Exception(\"No Azure regions available\")\n",
    "        except SystemExit:\n",
    "            raise SystemExit\n",
    "        except Exception as e:\n",
    "            if OPENAI_API_KEY:\n",
    "                try:\n",
    "                    openai.api_key = OPENAI_API_KEY\n",
    "                    openai.api_base = \"https://api.openai.com/v1\"\n",
    "                    openai.api_version = None\n",
    "                    openai.api_type = \"open_ai\"\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                        max_tokens=max_tokens,\n",
    "                        temperature=temperature,\n",
    "                        timeout=OPENAI_TIMEOUT,\n",
    "                        seed=SEED,\n",
    "                    )\n",
    "                    return response[\"choices\"][0].message.content\n",
    "                except SystemExit:\n",
    "                    raise SystemExit\n",
    "                except Exception as _e:\n",
    "                    logger.error(f\"OpenAI API Key found but error: {_e}\")\n",
    "            logger.error(f\"OpenAI API Key not found and Azure Error: {e}\")\n",
    "            # Raise exception to report error\n",
    "            raise e\n",
    "\"\"\")\n",
    "\n",
    "from rope.refactor import restructure\n",
    "pattern = r\"\"\"\n",
    "openai.api_key = ${api_key}\n",
    "openai.api_base = ${api_base}\n",
    "openai.api_version = ${api_version}\n",
    "openai.api_type = ${api_type}\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=${model},\n",
    "    messages=${messages},\n",
    "    max_tokens=${max_tokens},\n",
    "    temperature=${temperature},\n",
    "    timeout=${timeout},\n",
    "    seed=${seed},\n",
    ")\"\"\"\n",
    "goal = \"\"\"print(\n",
    "    ${api_key},\n",
    "    ${api_base},\n",
    "    ${api_version},\n",
    "    ${api_type},\n",
    "    ${model},\n",
    "    ${messages},\n",
    "    ${max_tokens},\n",
    "    ${temperature},\n",
    "    ${timeout},\n",
    "    ${seed},\n",
    ")\"\"\"\n",
    "\n",
    "restructuring = restructure.Restructure(myproject, pattern, goal)\n",
    "print(restructuring.get_changes().get_description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using function <_call_openai>:\n",
      "\n",
      "\n",
      "--- a/utils/openai_proxy.py\n",
      "+++ b/utils/openai_proxy.py\n",
      "@@ -60,18 +60,7 @@\n",
      "             ):\n",
      "                 engine = OPENAI_API_ENGINE_GPT4_32K\n",
      "             if OPENAI_API_TYPE is None or engine is None:\n",
      "-                openai.api_key = OPENAI_API_KEY\n",
      "-                openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                openai.api_version = None\n",
      "-                openai.api_type = \"open_ai\"\n",
      "-                response = openai.ChatCompletion.create(\n",
      "-                    model=model,\n",
      "-                    messages=messages,\n",
      "-                    max_tokens=max_tokens,\n",
      "-                    temperature=temperature,\n",
      "-                    timeout=OPENAI_TIMEOUT,\n",
      "-                    seed=SEED,\n",
      "-                )\n",
      "+                _call_openai(model, messages, max_tokens, temperature, OPENAI_API_KEY, \"https://api.openai.com/v1\", None, \"open_ai\")\n",
      "                 return response[\"choices\"][0].message.content\n",
      "             # validity checks for MULTI_REGION_CONFIG\n",
      "             if (\n",
      "@@ -130,18 +119,7 @@\n",
      "         except Exception as e:\n",
      "             if OPENAI_API_KEY:\n",
      "                 try:\n",
      "-                    openai.api_key = OPENAI_API_KEY\n",
      "-                    openai.api_base = \"https://api.openai.com/v1\"\n",
      "-                    openai.api_version = None\n",
      "-                    openai.api_type = \"open_ai\"\n",
      "-                    response = openai.ChatCompletion.create(\n",
      "-                        model=model,\n",
      "-                        messages=messages,\n",
      "-                        max_tokens=max_tokens,\n",
      "-                        temperature=temperature,\n",
      "-                        timeout=OPENAI_TIMEOUT,\n",
      "-                        seed=SEED,\n",
      "-                    )\n",
      "+                    _call_openai(model, messages, max_tokens, temperature, OPENAI_API_KEY, \"https://api.openai.com/v1\", None, \"open_ai\")\n",
      "                     return response[\"choices\"][0].message.content\n",
      "                 except SystemExit:\n",
      "                     raise SystemExit\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rope.refactor import usefunction\n",
    "\n",
    "sweep_project = rope.base.project.Project('../../sweepai', ignored_resources=[\"sandbox\"])\n",
    "resource = sweep_project.get_resource('utils/str_utils.py')\n",
    "# resource = sweep_project.get_resource('utils/openai_proxy.py')\n",
    "# start = resource.read().find('_call_openai')\n",
    "start = resource.read().find('entities_split')\n",
    "use_function = usefunction.UseFunction(sweep_project, resource, start)\n",
    "print(use_function.get_changes().get_description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from rope.base.project import Project\n",
    "from rope.contrib.autoimport import AutoImport\n",
    "\n",
    "project = Project(\"../../sweepai\")\n",
    "autoimport = AutoImport(project)\n",
    "# autoimport.generate_resource_cache()  # Generates a cache of the local modules, from the project you're working on\n",
    "autoimport.generate_modules_cache([\"../../sweepai\", \".\"])  # Generates a cache of external modules\n",
    "print(autoimport.import_assist(\"sweepai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('sweepai-u_CIt3kb-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d341f3248a096a89b9dbf6eec8e41f63aed02f6ba059df22a49224e3e8f1b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
